{"description": "A Zeek Unit Testing Framework", "tags": "library, unit-testing, testing", "version": "master", "depends": null, "test_cmd": "make -C tests", "build_cmd": null, "url": "https://github.com/corelight/ztest", "summary": null, "script_dir": "scripts", "plugin_dir": null, "readme": "# ZTest\nZTest - Zeek Unit Testing. Provides a framework to write unit tests for Zeek scripts.\n\n## Background and Example\nZTest is intended to be used alongside of your Zeek scripts to make your unit testing easy, fast, and more idiomatic. It can be combined with BTest as by default ZTest provides a rich output that clearly annotates test failures. This makes it easy to identify what failed in a BTest diff, for example. The basic structure of a ZTest file is as follows:\n\n```zeek\n@load ztest.zeek\n\nZTest::test_suite(\"Suite 1\");\n\nZTest::test(\"Example Test\", function() {\n    ZTest::assert_equal(2, 1, \"1 didn't equal 2\");\n});\n\nZTest::test(\"Another Example Test\", function() {\n    ZTest::assert_equal(\"hi\", \"hi\", \"Strings didn't match\");\n    ZTest::assert_instance_of(\"count\", \"Hi There\", \"Instance of didn't work\");\n    ZTest::assert_matches(/foo|bar/, \"hi there\", \"Didn't match when should have\");\n    ZTest::assert_greater_than(+15, +11, \"Greater than check failed\");\n});\n```\n\nThis will output the following (by default):\n```\nRunning Test Suite: Suite 1\n------------------------------------\nRunning Test: Example Test\n\nAssert Failed: 1 didn't equal 2\nExpected: 2\nActual: 1\n\nRunning Test: Another Example Test\n\nAssert Failed: Instance of didn't work\nExpected: count\nActual: string\n\nAssert Failed: Didn't match when should have\nExpected: 'hi there' to match pattern /^?(foo|bar)$?/\nActual: Didn't match pattern\n\nAssert Failed: Greater than check failed\nExpected: 11.0 to be greater than 15.0\nActual: Was less than or equal to\n\nTotal Assertions: 5\n    1 successful assertions\n    4 failed assertions\n```\n\n## General Test Structure\nTests are generally separated into different files for each test suite (but this is not a requirement). If you have more than one test file, it is recommended you also include a overall testing file that loads all of the tests so that they can be run altogether. Otherwise, you will have to invoke each file separately using zeek when you want to test particular parts of your codebase. \n\nInside of each test file, you use the `ZTest::test_suite(name: string)` method to let ZTest know that you are writing tests for a given suite. This helps you interpet the output, especially if you have many ZTest suites/files. If you don't specify a name, the name will be `<not defined>`. After the test suite name declaration, You write individual test cases by using the `ZTest::test_case(name: string, body: function)` method. This method takes an anonymous function that encapsulates your various test logic and your assert statements. When the tests are run, this anonymous function is executed and the results of the asserts can then be associated with the name of the test case. \n\nInside of a test_case (and technically outside of one too if you want), there are a number of assertions provided by the framework. These generally take the form of `assertion(expected, actual, failure_message)`\n\n* `assert`: A generic boolean assertion\n* `assert_equal`: Check that an actual value matches the expected value\n* `assert_instance_of`: Checks that the provided value is of a given Zeek variable type\n* `assert_matches`: Checks that a string matches a given regular expression\n* `assert_greater_than`: Checks that a number is greater than an expected threshold\n* `assert_greater_than_or_equal`: Checks that a number is greater than or equal to an expected threshold\n* `assert_less_than`: Checks that a number is less than an expected threshold\n* `assert_less_than_or_equal`: Checks that a number is less than or equal to an expected threshold\n* `assert_in_delta`: Checks that a number is within some delta value of a given threshold\n\nThe assertions will automatically keep track of what test case they belong to and whether they succeeded or failed. \n\n*NOTE*: Unlike some other test frameworks, ZTest will not end the testing when it encounters a failed assertion. This means that you shouldn't assume all assertions above the current one passed when you are writing your code.\n\n## Running The Tests\nZTest doesn't execute tests as soon as they are added using the `test` method. Instead, it waits for you to invoke them using the `ZTest::run_tests()` method. This is expected to be done at the end of each test suite. Once the tests have been run, the results are written to STDOUT (if enabled) and the current test suite is reset to an empty one. If you don't run the tests, ZTest will run them when zeek_exit() occurs.\n\n## Configuration Options and Special Methods\nZTest is designed to be used in a wide variety of manners, and as such has a few configuration options that you can use. These options are usually specified through calling functions that tell ZTest that you want to modify a configuration option. The methods are as follows:\n\n* `ZTest::suppress_success_output()`: Tells ZTest that it shouldn't write anything to STDOUT unless a test failed. This is a great method to call if you want to use ZTest with BTest since you can always add test cases without changing your benchmark (the benchmark becomes an empty STDOUT). If a test fails, it'll write to STDOUT and your BTest will fail since the benchmark is empty and the output won't be\n* `ZTest::suppress_all_output()`: Tells ZTest to not write anything to STDOUT (successes or failures). This is helpful if you want to do some custom work with the ZTest output (see `ZTest::retrieve_all_results`)\n* `ZTest::hook_exit()`: Tells ZTest to hook the `zeek_done` event and to alter the Zeek exit code based on the test results. If there were no failed tests, the exit code will be `0` and if there were any failed tests it will be `1`\n\nSpecial methods for interfacing with the ZTest framework are:\n\n* `ZTest::retrieve_all_results()`: Returns a Zeek table with all of the current results (for tests that have been run). The keys of the table are the names of the test suites and the values are another table with the key being each test name in that suite with the value being another table with the key being each assertion name and the value being T if the assertion passed and F if it did not:\n```\n{\n        [Suite 1] = {\n                [Another Test] = {\n                        [More examples] = T\n                },\n                [Example Test] = {\n                        [Should fail] = F,\n                        [1 didn't equal 1] = T\n                }\n        },\n        [Suite 3] = {\n                [Test me] = {\n                        [Empty String] = T\n                }\n        }\n}\n```\n* `ZTest::test_exit()`: Exits Zeek, setting the exit code to 0 if no tests failed and 1 if any test failed. This can be used to manually exit as opposed to enabling the auto exit hook with `ZTest::hook_exit()`\n\n## Integrating with BTest\nZTest is designed to work with BTest, not replace it. There are a couple of strategies for using ZTest in BTest:\n\n1. Write your unit tests and run them from BTest. Set the STDOUT benchmark file to be the output of the Unit Tests with all of them passing. If anything changes (such as a test failing) the output from ZTest will be different from the benchmark and BTest will show the test as failing. The downside to this approach is that if you add new tests you will have to update the benchmark.\n1. Write your unit tests and run them from BTest, but call `ZTest::suppress_success_output()` at some point in your test file. This will tell ZTest to only output errors to STDOUT. Set your BTest benchmark for STDOUT to be empty. If any test fails, it will have output and your BTest will fail because the output will not match the empty STDOUT benchmark. This is preferred since you can add any new unit tests and not have to update the benchmark, as long as they all pass.\n\n## Testing an Event-Driven System\nZeek is by its nature very event-driven. This can seemingly make unit testing much harder to do since a lot of work is done inside of event handlers. For example, given the following code:\n\n```zeek\nevent my_event(param1: string, param2: string) {\n    if (param1 == \"hi\" && param2 == \"there\") {\n        # Do something, maybe log a notice\n    }\n}\n```\n\nAt the surface, unit testing this seems very difficult. The reality is that good testing starts with code architecture. If we move our logic around we can now easily test it:\n\n```zeek\nfunction should_notice(param1: string, param2: string): bool {\n    return param1 == \"hi\" && param2 == \"there\";\n}\n\nevent my_event(param1: string, param2: string) {\n    if (should_notice(param1, param2)) {\n        # Do something, maybe log a notice\n    }\n}\n```\n\nWe can now test the `should_notice` function without having to worry about the event or logging parts of Zeek. This same pattern can (and should!) be used whenever possible to enable unit testing and to make more reusable code.\n\n## Installation\n### With Zeek Package Manager - Automatic\n`zkg install corelight/ztest`\n### With Zeek Package Manager - Manual Download\n```bash\ngit clone https://github.com/corelight/ztest\ncd ztest\nzkg install .\n```\n\n## Closing Thoughts\nThis framework was designed to better enable Zeek script developers to test their code. There are some things to consider with it, however:\n\n* Equality checks are executed by using the `cat` method internally. This means that for some container types (records, sets, and tables), the ordering of the elements in the container isn't always perfect internally in Zeek. This means that equality comparisons for those types aren't 100% accurate. If there are particular things you want to check, it is recommended you do so with more detailed inspection of the containers being compared\n* Events aren't handled or managed with ZTest. This means that you can't check if an event was raised or handled (yet). This is a future desired feature but something to remember\n* Test cases are inside of anonymous functions. Take care with scoping and closures\n\n### Future Ideas/Plans/Wish list:\n* Event-aware assertions such as `assert_raises(event)` or `assert_handles(event)`\n* Logging or Notice framework aware assertions such as `assert_generates_notice` or `assert_logs`\n* Broker aware assertions (publishing to topic, etc.)\n\n## Running the ZTest Framework Tests\nZTest comes with unit tests for itself. These are a set of Zeek scripts that exercise the functionality of the framework. The unit tests are run using a Ruby driver. Ruby is only used to unit test ZTest, and isn't a requirement for using ZTest otherwise. To run the tests:\n```bash\ncd tests\nruby test_ztest.rb\n```\n"}